# scripts/run_eval.py
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from datasets import load_dataset, Dataset
from evaluate import load as load_metric
import torch
from tqdm import tqdm

# æ¨¡å‹è·¯å¾„
model_path = "models/distilbert-qa"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForQuestionAnswering.from_pretrained(model_path)

# åŠ è½½åŸå§‹ SQuAD æ ¼å¼æ•°æ®å¹¶æ‰å¹³åŒ–
raw_dataset = load_dataset("json", data_files="data/squad/clean_train.json", field="data")["train"]

samples = []
for item in raw_dataset:
    for para in item["paragraphs"]:
        context = para["context"]
        for qa in para["qas"]:
            question = qa["question"]
            answer = qa["answers"][0]
            samples.append({
                "context": context,
                "question": question,
                "answer_text": answer["text"],
                "answer_start": answer["answer_start"]
            })

# æ„å»ºéªŒè¯é›†ï¼ˆéšæœº 10%ï¼‰
dataset = Dataset.from_list(samples).train_test_split(test_size=0.1)["test"]

# åŠ è½½è¯„ä¼°æŒ‡æ ‡
metric = load_metric("squad")

# æ¨ç†å‡½æ•°
def get_answer(example):
    inputs = tokenizer(
        example["question"],
        example["context"],
        return_tensors="pt",
        truncation=True,
        padding=True
    )
    with torch.no_grad():
        outputs = model(**inputs)

    start_idx = torch.argmax(outputs.start_logits)
    end_idx = torch.argmax(outputs.end_logits) + 1

    answer_tokens = inputs["input_ids"][0][start_idx:end_idx]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
    return answer.strip()

# é¢„æµ‹ + å¯¹æ¯”çœŸå®ç­”æ¡ˆ
predictions = []
references = []

print("ğŸ§ª æ­£åœ¨è¯„ä¼°æ¨¡å‹...")
for i, example in enumerate(tqdm(dataset)):
    pred = get_answer(example)
    ref = {
        "id": str(i),
        "answers": {
            "answer_start": [example["answer_start"]],
            "text": [example["answer_text"]]
        }
    }
    predictions.append({"id": str(i), "prediction_text": pred})
    references.append(ref)

# è®¡ç®—åˆ†æ•°
results = metric.compute(predictions=predictions, references=references)
print("\nâœ… Evaluation Result:")
print(f"ğŸ“Œ Exact Match: {results['exact_match']:.2f}")
print(f"ğŸ“Œ F1 Score:    {results['f1']:.2f}")