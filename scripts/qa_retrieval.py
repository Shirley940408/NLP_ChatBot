# scripts/qa_retrieval.py

import torch
import faiss
import json
import numpy as np
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from sentence_transformers import SentenceTransformer

# åŠ è½½æ¨¡å‹ï¼ˆä½  fine-tune çš„ QA æ¨¡å‹ï¼‰
model_path = "models/distilbert-qa"
qa_tokenizer = AutoTokenizer.from_pretrained(model_path)
qa_model = AutoModelForQuestionAnswering.from_pretrained(model_path)

# æ£€ç´¢æ¨¡å‹
encoder = SentenceTransformer("all-MiniLM-L6-v2")
index = faiss.read_index("retrieval/context.index")
with open("retrieval/contexts.json", "r") as f:
    all_contexts = json.load(f)

# ä¸»å‡½æ•°
def qa_with_auto_context(question, top_k=1):
    # ç¼–ç é—®é¢˜
    q_vec = encoder.encode([question], convert_to_numpy=True)

    # åœ¨ç´¢å¼•ä¸­æœç´¢
    D, I = index.search(q_vec, top_k)
    context = all_contexts[I[0][0]]

    # QA æ¨ç†
    inputs = qa_tokenizer(question, context, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = qa_model(**inputs)

    start_idx = torch.argmax(outputs.start_logits)
    end_idx = torch.argmax(outputs.end_logits) + 1

    answer_tokens = inputs["input_ids"][0][start_idx:end_idx]
    answer = qa_tokenizer.decode(answer_tokens, skip_special_tokens=True)

    return {
        "question": question,
        "context": context,
        "answer": answer.strip()
    }

# æµ‹è¯•ï¼š
if __name__ == "__main__":
    q = "What did God create?"
    result = qa_with_auto_context(q)
    print(f"â“ {result['question']}")
    print(f"ğŸ“š Context: {result['context'][:100]}...")
    print(f"âœ… Answer: {result['answer']}")
