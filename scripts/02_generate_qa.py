# scripts/02_generate_qa.py
import os
import json
import spacy
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering

# ===== é…ç½®å‚æ•° =====
INPUT_FOLDER = "data/raw"
OUTPUT_FILE = "data/squad/train.json"
QUESTION_GEN_MODEL = "google/flan-t5-small"
ANSWER_MODEL = "distilbert-base-cased-distilled-squad"

# ===== åŠ è½½ SpaCy åˆ†å¥å™¨ =====
nlp = spacy.load("en_core_web_sm")
nlp.max_length = 10_000_000

# ===== åŠ è½½é—®ç­”æ¨¡å‹ï¼ˆç”¨äºæå–ç­”æ¡ˆï¼‰=====
print("Loading QA model...")
qa_pipeline = pipeline("question-answering", model=ANSWER_MODEL, tokenizer=ANSWER_MODEL, device=-1)

# ===== åŠ è½½æé—®æ¨¡å‹ =====
print("Loading question generation model...")
qgen_pipeline = pipeline("text2text-generation", model=QUESTION_GEN_MODEL, tokenizer=QUESTION_GEN_MODEL, device=-1)

# ===== åˆå§‹åŒ–æ•°æ®ç»“æ„ =====
data = {"data": []}

# ===== ç”Ÿæˆé—®é¢˜çš„å‡½æ•°ï¼ˆç”¨ T5ï¼‰=====
def generate_question_t5(text_chunk):
    prompt = f"Generate a question from the following passage:\n\n{text_chunk.strip()}"
    try:
        response = qgen_pipeline(prompt, max_new_tokens=64, do_sample=False)[0]["generated_text"]
        return response.strip()
    except Exception as e:
        print("âš ï¸ Failed to generate question:", e)
        return None

# ===== æ„å»º SQuAD æ ¼å¼è®°å½• =====
def build_squad_record(context, question, answer_dict, idx):
    answer_text = answer_dict['answer']
    start_idx = answer_dict['start']
    if not answer_text or start_idx == -1:
        print("âš ï¸ Answer not found in context.")
        return None

    return {
        "context": context,
        "qas": [{
            "id": f"q{idx}",
            "question": question,
            "answers": [{"text": answer_text, "answer_start": start_idx}],
            "is_impossible": False
        }]
    }

# ===== ä¸»æµç¨‹ =====
idx = 0
for fname in os.listdir(INPUT_FOLDER):
    if fname.endswith(".txt"):
        with open(os.path.join(INPUT_FOLDER, fname), "r", encoding="utf-8") as f:
            text = f.read()
            doc = nlp(text)
            sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 0]
            chunks, current = [], ""
            for sent in sentences:
                if len(current) + len(sent) < 500:
                    current += sent + " "
                else:
                    if len(current.strip()) >= 100:
                        chunks.append(current.strip())
                    current = sent + " "
            if len(current.strip()) >= 100:
                chunks.append(current.strip())

            # é™åˆ¶æ¯ä¸ªæ–‡ä»¶æœ€å¤šç”Ÿæˆ 10 ä¸ªé—®ç­”å¯¹
            for i, chunk in enumerate(chunks):
                if i >= 500:
                    break
                print("ğŸ“– Generating question for:", chunk[:80].replace("\n", " ") + "...")
                question = generate_question_t5(chunk)
                if not question:
                    continue
                print("â“ Question:", question)
                try:
                    result = qa_pipeline(question=question, context=chunk)
                    print("âœ… Answer:", result["answer"])
                    record = build_squad_record(chunk, question, result, idx)
                    if record:
                        data["data"].append({"title": fname, "paragraphs": [record]})
                        idx += 1
                    else:
                        print("âŒ Could not build a valid QA record")
                except Exception as e:
                    print(f"âŒ Failed to extract answer: {e}")

# ===== å†™å…¥è¾“å‡º =====
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(data, f, indent=2)

print(f"\nğŸ‰ Done! Generated {idx} QA pairs. Output saved to {OUTPUT_FILE}")
