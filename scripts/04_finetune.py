# scripts/04_finetune.py

from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer
import os

# å¼ºåˆ¶ä½¿ç”¨ CPU
os.environ["CUDA_VISIBLE_DEVICES"] = ""

# ===== é…ç½®æ¨¡å‹å’Œ tokenizer =====
model_name = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

# ===== åŠ è½½æ¸…æ´—å¥½çš„ SQuAD æ ¼å¼æ•°æ® =====
dataset = load_dataset("json", data_files={"train": "data/squad/clean_train.json"}, field="data")

# ===== æ‰å¹³åŒ–æˆå•ä¸ªæ ·æœ¬æ ¼å¼ =====
samples = []
for item in dataset["train"]:
    for paragraph in item["paragraphs"]:
        context = paragraph["context"]
        for qa in paragraph["qas"]:
            question = qa["question"]
            answer = qa["answers"][0]
            answer_text = answer["text"]
            answer_start = answer["answer_start"]
            if context[answer_start:answer_start + len(answer_text)] != answer_text:
                continue  # ç­”æ¡ˆä½ç½®ä¸åŒ¹é…ï¼Œè·³è¿‡
            samples.append({
                "context": context,
                "question": question,
                "answer_text": answer_text,
                "answer_start": answer_start
            })

dataset = Dataset.from_list(samples)

# ===== åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† =====
split_dataset = dataset.train_test_split(test_size=0.1)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]

# ===== é¢„å¤„ç†å‡½æ•°ï¼šæ„å»º token çº§åˆ«çš„ answer span =====
def preprocess(example):
    tokenized = tokenizer(
        example["question"],
        example["context"],
        max_length=384,
        truncation="only_second",  # ä¿è¯æˆªæ–­å‘ç”Ÿåœ¨ context éƒ¨åˆ†
        padding="max_length",
        return_offsets_mapping=True
    )

    start_char = example["answer_start"]
    end_char = start_char + len(example["answer_text"])
    offsets = tokenized["offset_mapping"]

    start_token = end_token = None

    for idx, (start, end) in enumerate(offsets):
        if start <= start_char < end:
            start_token = idx
        if start < end_char <= end:
            end_token = idx

    # fallbackï¼šæ‰¾ä¸åˆ°å°±è®¾ä¸º0ï¼Œé˜²æ­¢è®­ç»ƒcrash
    if start_token is None or end_token is None:
        start_token = end_token = 0

    tokenized.update({
        "start_positions": start_token,
        "end_positions": end_token
    })
    tokenized.pop("offset_mapping")  # ä¸ä¿ç•™ offset

    return tokenized

# ===== Tokenize æ•°æ® =====
train_tokenized = train_dataset.map(preprocess, remove_columns=train_dataset.column_names)
eval_tokenized = eval_dataset.map(preprocess, remove_columns=eval_dataset.column_names)

# ===== è®¾ç½®è®­ç»ƒå‚æ•° =====
training_args = TrainingArguments(
    output_dir="models/distilbert-qa",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="logs",
    logging_steps=50,
    num_train_epochs=4,
    per_device_train_batch_size=8,
    save_total_limit=1,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    report_to="none"
)

# ===== Trainer =====
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=eval_tokenized,
    tokenizer=tokenizer
)

# ===== å¼€å§‹è®­ç»ƒ =====
print("ğŸš€ å¼€å§‹ fine-tune...")
trainer.train()
tokenizer.save_pretrained("models/distilbert-qa")
print("\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ° models/distilbert-qa")